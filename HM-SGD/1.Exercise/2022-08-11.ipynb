{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue\" align=\"center\">Plain Deep Neural Network (DNN)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import time, math \n",
    "\n",
    "from tensorflow import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "from keras.callbacks import Callback, CSVLogger "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and split the dataset into training and testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST - Handwritten digits recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(X_train,y_train),(X_test,y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/MNIST-Handwritten digits.png\" height=450 width=450/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST - Fashion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/MNIST-fashion.png\" height=400 width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For MNIST handwritten digits and Fashion dataset\n",
    "X_train_flattened = X_train.reshape(len(X_train), 28 * 28)\n",
    "X_test_flattened = X_test.reshape(len(X_test),  28 * 28) \n",
    "\n",
    "# For CIFAR-10 and CIFAR-100\n",
    "#X_train_flattened = X_train.reshape(len(X_train), 32 * 32)\n",
    "#X_test_flattened = X_test.reshape(len(X_test),  32 * 32) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized = X_train_flattened / 255\n",
    "X_test_normalized = X_test_flattened / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define DNN model with two layers, optimizer, metrics, and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "def get_model(): \n",
    "    model = Sequential([\n",
    "        # input layer 784 neurons to first hidden layer with 64 neurons\n",
    "        Dense(64, input_shape = (784,), activation='relu'), \n",
    "        # first hidden layer to second hidden layer\n",
    "        Dense(64, activation='relu'),  \n",
    "        # Output layer with 10 neurons\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # General SGD\n",
    "    #opti = keras.optimizers.SGD(learning_rate=0.01)\n",
    "    \n",
    "    # SGD with momentum\n",
    "    #opti = keras.optimizers.SGD(learning_rate=0.01, momentum=0.6)\n",
    "    \n",
    "    # SGD with Nesterov momentum \n",
    "    #opti = keras.optimizers.SGD(learning_rate=0.01, momentum=0.6, nesterov=True)\n",
    "    \n",
    "    # RMSprop \n",
    "    #opti = keras.optimizers.RMSprop(learning_rate=0.001, momentum=0.6)\n",
    "    \n",
    "    # Adam\n",
    "    opti = keras.optimizers.Adam(learning_rate=0.001) \n",
    "    \n",
    "    # Adamax\n",
    "    #opti = keras.optimizers.Adamax(learning_rate=0.001) \n",
    "    \n",
    "    model.compile(\n",
    "        optimizer = opti,\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For generic optimizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallbackGeneric(Callback):  \n",
    "    # Training stop criteria\n",
    "    stop_at = 0.99\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')> self.stop_at):  \n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For HM based optimizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallbackHM(Callback):  \n",
    "    # Stop the algorithm when the following accuracy reached \n",
    "    stop_at = 0.99\n",
    "    \n",
    "    # to update weights for batch gradient desecent\n",
    "    batch_size = 60000    \n",
    "    iteration = 0    \n",
    "    training_set_size = 0\n",
    "    update_every_fold = 0  \n",
    "    \n",
    "    initial_weights = 0\n",
    "    previous_weights = 0\n",
    "    call_hm = 0 \n",
    "     \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.initial_weights = model_hm.get_weights() \n",
    "        self.initial_weights = np.array(self.initial_weights,dtype=object)\n",
    "        self.previous_weights = self.initial_weights\n",
    "        # Harmonic mean based weights calculation\n",
    "        self.call_hm = np.vectorize(self.apply_hm) \n",
    "        #Determining number of updates for mini-batch gradient every epoch\n",
    "        self.training_set_size = X_train_normalized.shape[0]\n",
    "        self.update_every_fold = self.fold_calc(self.batch_size, self.training_set_size)\n",
    "             \n",
    "    def on_epoch_begin(self, epoch,  logs=None): \n",
    "        self.iteration = 1   \n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs=None): \n",
    "        if self.iteration%self.update_every_fold == 0:  \n",
    "            counter = 0\n",
    "            num_layers = len(model_hm.layers)  \n",
    "            current_weights = model_hm.get_weights()\n",
    "            current_weights = np.array(current_weights,dtype=object)        \n",
    "\n",
    "            for i in range(num_layers):  \n",
    "                # Harmonic mean based weights calculation\n",
    "                current_weights[counter] = self.call_hm(self.previous_weights[counter], current_weights[counter])\n",
    "                counter = counter + 2\n",
    "            \n",
    "            # Updating the model with new weights\n",
    "            updated = current_weights.tolist()   \n",
    "            model_hm.set_weights(updated)\n",
    "            self.previous_weights = current_weights\n",
    "        self.iteration = self.iteration + 1  \n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Training stop criteria\n",
    "        if(logs.get('accuracy')> self.stop_at):  \n",
    "            self.model.stop_training = True\n",
    "\n",
    "    def apply_hm(self, v1, v2):     \n",
    "        if v1==0 or v2==0:\n",
    "            return v2\n",
    "        elif v1>0 and v2>0:\n",
    "            hm = 2*v1*v2/(v1+v2)\n",
    "            min1 = min(v1,v2)\n",
    "            diff = abs(hm-min1)\n",
    "            if v2 > v1:\n",
    "                return v2 + diff\n",
    "            else:\n",
    "                return v2 - diff\n",
    "        elif v1<0 and v2<0:\n",
    "            hm = 2*v1*v2/(v1+v2)\n",
    "            max1 = max(v1,v2)\n",
    "            diff = abs(hm-max1)\n",
    "            if v2 > v1:\n",
    "                return v2 + diff\n",
    "            else:\n",
    "                return v2 - diff\n",
    "        else:\n",
    "            return v2  \n",
    "        \n",
    "    def fold_calc(self, batch_size, training_set_size): \n",
    "        total_fold = int(math.ceil(math.log2(training_set_size)))\n",
    "        #print(\"total_fold   :\", total_fold) \n",
    "        num_batch = math.ceil(training_set_size / batch_size)\n",
    "        #print(\"num_batch   :\", num_batch)\n",
    "        batch_fold = int(math.floor(math.log2(batch_size)))\n",
    "        #print(\"batch_fold   :\", batch_fold)\n",
    "        update_every_fold = (total_fold-batch_fold)    \n",
    "        #print(\"update_every_fold   :\", update_every_fold)\n",
    "        num_updates = num_batch/update_every_fold\n",
    "        #print(\"num_updates   :\", num_updates)\n",
    "        return update_every_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To record loss and accuracy in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger_generic_model = CSVLogger('Generic_model_MNIST.csv', append=False, separator=',')\n",
    "logger_hm_model = CSVLogger('HM_model_MNIST.csv', append=False, separator=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic opimizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 663ms/step - loss: 2.3085 - accuracy: 0.1054\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 2.1792 - accuracy: 0.2074\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 2.0838 - accuracy: 0.3064\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 1.9965 - accuracy: 0.3773\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 1.9103 - accuracy: 0.4345\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 1.8247 - accuracy: 0.4777\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 1.7399 - accuracy: 0.5163\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 1.6573 - accuracy: 0.5542\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 1.5775 - accuracy: 0.5867\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 1.5008 - accuracy: 0.6072\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 1.4282 - accuracy: 0.6201\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 1.3604 - accuracy: 0.6251\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 1.2968 - accuracy: 0.6314\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 1.2367 - accuracy: 0.6372\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 1.1804 - accuracy: 0.6444\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 1.1287 - accuracy: 0.6500\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 1.0807 - accuracy: 0.6560\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 1.0355 - accuracy: 0.6651\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.9934 - accuracy: 0.6779\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.9550 - accuracy: 0.6921\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.9204 - accuracy: 0.7029\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.8893 - accuracy: 0.7089\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.8609 - accuracy: 0.7139\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.8348 - accuracy: 0.7167\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.8122 - accuracy: 0.7203\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.7920 - accuracy: 0.7243\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.7735 - accuracy: 0.7298\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.7564 - accuracy: 0.7366\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.7406 - accuracy: 0.7446\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.7262 - accuracy: 0.7509\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.7123 - accuracy: 0.7570\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.6993 - accuracy: 0.7622\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.6874 - accuracy: 0.7663\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.6763 - accuracy: 0.7700\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.6657 - accuracy: 0.7751\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.6556 - accuracy: 0.7799\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.6458 - accuracy: 0.7844\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.6365 - accuracy: 0.7870\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.6276 - accuracy: 0.7903\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.6192 - accuracy: 0.7921\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.6110 - accuracy: 0.7945\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.6031 - accuracy: 0.7970\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.5956 - accuracy: 0.7995\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.5885 - accuracy: 0.8017\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.5817 - accuracy: 0.8036\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.5753 - accuracy: 0.8053\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.5692 - accuracy: 0.8073\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.5634 - accuracy: 0.8091\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.5579 - accuracy: 0.8109\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.5527 - accuracy: 0.8128\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.5476 - accuracy: 0.8145\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.5427 - accuracy: 0.8159\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.5380 - accuracy: 0.8178\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.5336 - accuracy: 0.8191\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.5294 - accuracy: 0.8203\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.5253 - accuracy: 0.8214\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.5213 - accuracy: 0.8225\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.5175 - accuracy: 0.8245\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.5139 - accuracy: 0.8254\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.5103 - accuracy: 0.8260\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.5069 - accuracy: 0.8268\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.5035 - accuracy: 0.8276\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.5003 - accuracy: 0.8283\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.4972 - accuracy: 0.8294\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.4941 - accuracy: 0.8301\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.4912 - accuracy: 0.8312\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.4884 - accuracy: 0.8322\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.4856 - accuracy: 0.8331\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.4830 - accuracy: 0.8334\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.4804 - accuracy: 0.8343\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.4779 - accuracy: 0.8351\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.4754 - accuracy: 0.8363\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.4731 - accuracy: 0.8367\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.4708 - accuracy: 0.8373\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.4686 - accuracy: 0.8383\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.4665 - accuracy: 0.8385\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.4648 - accuracy: 0.8394\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.4641 - accuracy: 0.8391\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.4628 - accuracy: 0.8398\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.4602 - accuracy: 0.8404\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.4567 - accuracy: 0.8422\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.4562 - accuracy: 0.8420\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 186ms/step - loss: 0.4553 - accuracy: 0.8424\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.4517 - accuracy: 0.8439\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.4510 - accuracy: 0.8440\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.4502 - accuracy: 0.8439\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.4469 - accuracy: 0.8456\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.4466 - accuracy: 0.8452\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.4454 - accuracy: 0.8457\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.4424 - accuracy: 0.8467\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.4425 - accuracy: 0.8465\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.4407 - accuracy: 0.8472\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.4384 - accuracy: 0.8479\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.4385 - accuracy: 0.8479\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.4362 - accuracy: 0.8484\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.4346 - accuracy: 0.8489\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.4343 - accuracy: 0.8490\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.4321 - accuracy: 0.8499\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.4311 - accuracy: 0.8503\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.4303 - accuracy: 0.8507\n",
      "Execution time: 17.981179237365723 seconds\n"
     ]
    }
   ],
   "source": [
    "model_wihtout_hm = get_model() \n",
    "st = time.time()\n",
    "model_wihtout_hm.fit(X_train_normalized, y_train, epochs = 100, verbose=1, callbacks=[CustomCallbackGeneric(), logger_generic_model], batch_size=60000) \n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HM based optimizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 718ms/step - loss: 2.3422 - accuracy: 0.1530\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.1698 - accuracy: 0.2272\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.0489 - accuracy: 0.3683\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 1.9210 - accuracy: 0.4486\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 1.7834 - accuracy: 0.5155\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 1.6441 - accuracy: 0.5777\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 1.5142 - accuracy: 0.6190\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 1.3960 - accuracy: 0.6402\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 1.2885 - accuracy: 0.6523\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 1.1930 - accuracy: 0.6600\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 1.1094 - accuracy: 0.6644\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 1.0386 - accuracy: 0.6669\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.9795 - accuracy: 0.6723\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.9294 - accuracy: 0.6772\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.8877 - accuracy: 0.6855\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.8534 - accuracy: 0.6942\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.8248 - accuracy: 0.7031\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.8003 - accuracy: 0.7099\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.7796 - accuracy: 0.7145\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.7616 - accuracy: 0.7182\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.7459 - accuracy: 0.7225\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.7315 - accuracy: 0.7277\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.7183 - accuracy: 0.7341\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.7058 - accuracy: 0.7405\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.6938 - accuracy: 0.7449\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.6824 - accuracy: 0.7501\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.6711 - accuracy: 0.7567\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.6604 - accuracy: 0.7635\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.6503 - accuracy: 0.7687\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.6403 - accuracy: 0.7726\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.6307 - accuracy: 0.7761\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.6216 - accuracy: 0.7806\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.6129 - accuracy: 0.7858\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.6045 - accuracy: 0.7901\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.5966 - accuracy: 0.7926\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.5891 - accuracy: 0.7950\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.5820 - accuracy: 0.7966\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.5753 - accuracy: 0.7995\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.5689 - accuracy: 0.8023\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.5629 - accuracy: 0.8053\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.5571 - accuracy: 0.8077\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.5516 - accuracy: 0.8088\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.5464 - accuracy: 0.8107\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.5413 - accuracy: 0.8131\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.5362 - accuracy: 0.8151\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.5313 - accuracy: 0.8171\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.5265 - accuracy: 0.8188\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.5219 - accuracy: 0.8200\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.5175 - accuracy: 0.8213\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.5132 - accuracy: 0.8232\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.5091 - accuracy: 0.8247\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.5052 - accuracy: 0.8261\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.5019 - accuracy: 0.8265\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.4989 - accuracy: 0.8288\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.4953 - accuracy: 0.8281\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.4907 - accuracy: 0.8304\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.4871 - accuracy: 0.8318\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.4848 - accuracy: 0.8322\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.4817 - accuracy: 0.8340\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.4778 - accuracy: 0.8352\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.4751 - accuracy: 0.8360\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.4728 - accuracy: 0.8367\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.4697 - accuracy: 0.8373\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.4667 - accuracy: 0.8387\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.4646 - accuracy: 0.8394\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.4623 - accuracy: 0.8394\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.4595 - accuracy: 0.8410\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.4572 - accuracy: 0.8419\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.4553 - accuracy: 0.8419\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.4530 - accuracy: 0.8436\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.4506 - accuracy: 0.8440\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.4486 - accuracy: 0.8448\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.4468 - accuracy: 0.8459\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.4446 - accuracy: 0.8462\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.4425 - accuracy: 0.8470\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.4408 - accuracy: 0.8476\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.4390 - accuracy: 0.8480\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.4370 - accuracy: 0.8488\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.4351 - accuracy: 0.8493\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.4334 - accuracy: 0.8503\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.4318 - accuracy: 0.8506\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.4300 - accuracy: 0.8515\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 208ms/step - loss: 0.4282 - accuracy: 0.8515\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.4266 - accuracy: 0.8521\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.4250 - accuracy: 0.8528\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.4234 - accuracy: 0.8531\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.4218 - accuracy: 0.8538\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.4201 - accuracy: 0.8539\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.4186 - accuracy: 0.8545\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.4173 - accuracy: 0.8546\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.4166 - accuracy: 0.8545\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.4173 - accuracy: 0.8541\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.4168 - accuracy: 0.8544\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.4142 - accuracy: 0.8558\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.4105 - accuracy: 0.8573\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.4110 - accuracy: 0.8560\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.4101 - accuracy: 0.8568\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.4064 - accuracy: 0.8587\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.4074 - accuracy: 0.8572\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.4059 - accuracy: 0.8582\n",
      "Execution time: 21.89090919494629 seconds\n"
     ]
    }
   ],
   "source": [
    "model_hm = get_model() \n",
    "st = time.time()\n",
    "model_hm.fit(X_train_normalized, y_train, epochs = 100, verbose=1, callbacks=[CustomCallbackHM(),logger_hm_model], batch_size=60000) \n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_51 (Dense)            (None, 64)                50240     \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_hm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic opimizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 994us/step - loss: 0.4637 - accuracy: 0.8381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.46369126439094543, 0.838100016117096]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wihtout_hm.evaluate(X_test_normalized, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HM based optimizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 987us/step - loss: 0.4384 - accuracy: 0.8482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4384474754333496, 0.8482000231742859]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_hm.evaluate(X_test_normalized, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
