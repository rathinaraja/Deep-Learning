{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue\" align=\"center\">Deep Neural Network (DNN) on MNIST</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import time, math \n",
    "\n",
    "from tensorflow import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "from keras.callbacks import Callback, CSVLogger "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and split the dataset into training and testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST - Handwritten digits recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(X_train,y_train),(X_test,y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST - Fashion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flattened = X_train.reshape(len(X_train), 28 * 28)\n",
    "X_test_flattened = X_test.reshape(len(X_test),  28 * 28) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized = X_train_flattened / 255\n",
    "X_test_normalized = X_test_flattened / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define DNN model with two layers, optimizer, metrics, and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "def get_model(): \n",
    "    model = Sequential([\n",
    "        # input layer 784 neurons to first hidden layer with 64 neurons\n",
    "        Dense(64, input_shape = (784,), activation='relu'), \n",
    "        # first hidden layer to second hidden layer\n",
    "        Dense(64, activation='relu'),  \n",
    "        # Output layer with 10 neurons\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # General SGD\n",
    "    #opti = keras.optimizers.SGD(learning_rate=0.01)\n",
    "    \n",
    "    # SGD with momentum\n",
    "    #opti = keras.optimizers.SGD(learning_rate=0.01, momentum=0.6)\n",
    "    \n",
    "    # SGD with Nesterov momentum \n",
    "    #opti = keras.optimizers.SGD(learning_rate=0.01, momentum=0.6, nesterov=True)\n",
    "    \n",
    "    # RMSprop \n",
    "    #opti = keras.optimizers.RMSprop(learning_rate=0.001, momentum=0.6)\n",
    "    \n",
    "    # Adam\n",
    "    opti = keras.optimizers.Adam(learning_rate=0.001) \n",
    "    \n",
    "    # Adamax\n",
    "    #opti = keras.optimizers.Adamax(learning_rate=0.001) \n",
    "    \n",
    "    model.compile(\n",
    "        optimizer = opti,\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For generic optimizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallbackGeneric(Callback):  \n",
    "    # Training stop criteria\n",
    "    stop_at = 0.99\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')> self.stop_at):  \n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For HM based optimizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallbackHM(Callback):  \n",
    "    # Stop the algorithm when the following accuracy reached \n",
    "    stop_at = 0.995  \n",
    "    \n",
    "    initial_weights = 0\n",
    "    previous_weights = 0\n",
    "    call_hm = 0 \n",
    "     \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.initial_weights = model_hm.get_weights() \n",
    "        self.initial_weights = np.array(self.initial_weights,dtype=object)\n",
    "        self.previous_weights = self.initial_weights\n",
    "        # Harmonic mean based weights calculation\n",
    "        self.call_hm = np.vectorize(self.apply_hm)  \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Training stop criteria\n",
    "        if(logs.get('accuracy')> self.stop_at):  \n",
    "            self.model.stop_training = True\n",
    "        \n",
    "        counter = 0\n",
    "        num_layers = len(model_hm.layers)  \n",
    "        current_weights = model_hm.get_weights()\n",
    "        current_weights = np.array(current_weights,dtype=object)        \n",
    "\n",
    "        for i in range(num_layers):  \n",
    "            # Harmonic mean based weights calculation\n",
    "            current_weights[counter] = self.call_hm(self.previous_weights[counter], current_weights[counter])\n",
    "            counter = counter + 2\n",
    "            \n",
    "        # Updating the model with new weights\n",
    "        updated = current_weights.tolist()   \n",
    "        model_hm.set_weights(updated)\n",
    "        self.previous_weights = current_weights\n",
    "        \n",
    "    def apply_hm(self, v1, v2):     \n",
    "        if v1==0 or v2==0:\n",
    "            return v2\n",
    "        elif v1>0 and v2>0:\n",
    "            hm = 2*v1*v2/(v1+v2)\n",
    "            min1 = min(v1,v2)\n",
    "            diff = abs(hm-min1)\n",
    "            if v2 > v1:\n",
    "                return v2 + diff\n",
    "            else:\n",
    "                return v2 - diff\n",
    "        elif v1<0 and v2<0:\n",
    "            hm = 2*v1*v2/(v1+v2)\n",
    "            max1 = max(v1,v2)\n",
    "            diff = abs(hm-max1)\n",
    "            if v2 > v1:\n",
    "                return v2 + diff\n",
    "            else:\n",
    "                return v2 - diff\n",
    "        else:\n",
    "            return v2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To record loss and accuracy in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger_generic_model = CSVLogger('Generic_model_MNIST.csv', append=False, separator=',')\n",
    "logger_hm_model = CSVLogger('HM_model_MNIST.csv', append=False, separator=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic optimizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/135\n",
      "1/1 [==============================] - 1s 722ms/step - loss: 2.3702 - accuracy: 0.1018\n",
      "Epoch 2/135\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 2.2419 - accuracy: 0.1424\n",
      "Epoch 3/135\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 2.1498 - accuracy: 0.2135\n",
      "Epoch 4/135\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 2.0718 - accuracy: 0.2849\n",
      "Epoch 5/135\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 1.9945 - accuracy: 0.3627\n",
      "Epoch 6/135\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 1.9146 - accuracy: 0.4346\n",
      "Epoch 7/135\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 1.8331 - accuracy: 0.4964\n",
      "Epoch 8/135\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 1.7523 - accuracy: 0.5329\n",
      "Epoch 9/135\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 1.6711 - accuracy: 0.5582\n",
      "Epoch 10/135\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 1.5898 - accuracy: 0.5765\n",
      "Epoch 11/135\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 1.5101 - accuracy: 0.5946\n",
      "Epoch 12/135\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 1.4325 - accuracy: 0.6122\n",
      "Epoch 13/135\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 1.3577 - accuracy: 0.6301\n",
      "Epoch 14/135\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 1.2877 - accuracy: 0.6486\n",
      "Epoch 15/135\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 1.2237 - accuracy: 0.6649\n",
      "Epoch 16/135\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 1.1651 - accuracy: 0.6770\n",
      "Epoch 17/135\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 1.1113 - accuracy: 0.6880\n",
      "Epoch 18/135\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 1.0621 - accuracy: 0.6961\n",
      "Epoch 19/135\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 1.0172 - accuracy: 0.6995\n",
      "Epoch 20/135\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.9765 - accuracy: 0.7014\n",
      "Epoch 21/135\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.9400 - accuracy: 0.7031\n",
      "Epoch 22/135\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.9073 - accuracy: 0.7057\n",
      "Epoch 23/135\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.8781 - accuracy: 0.7102\n",
      "Epoch 24/135\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.8514 - accuracy: 0.7165\n",
      "Epoch 25/135\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.8271 - accuracy: 0.7240\n",
      "Epoch 26/135\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.8054 - accuracy: 0.7304\n",
      "Epoch 27/135\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.7857 - accuracy: 0.7358\n",
      "Epoch 28/135\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.7677 - accuracy: 0.7429\n",
      "Epoch 29/135\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.7512 - accuracy: 0.7501\n",
      "Epoch 30/135\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.7362 - accuracy: 0.7555\n",
      "Epoch 31/135\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.7224 - accuracy: 0.7588\n",
      "Epoch 32/135\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.7097 - accuracy: 0.7615\n",
      "Epoch 33/135\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.6980 - accuracy: 0.7642\n",
      "Epoch 34/135\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.6872 - accuracy: 0.7665\n",
      "Epoch 35/135\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.6770 - accuracy: 0.7693\n",
      "Epoch 36/135\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.6673 - accuracy: 0.7717\n",
      "Epoch 37/135\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.6580 - accuracy: 0.7747\n",
      "Epoch 38/135\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.6492 - accuracy: 0.7793\n",
      "Epoch 39/135\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.6408 - accuracy: 0.7829\n",
      "Epoch 40/135\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.6328 - accuracy: 0.7863\n",
      "Epoch 41/135\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.6252 - accuracy: 0.7891\n",
      "Epoch 42/135\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.6178 - accuracy: 0.7915\n",
      "Epoch 43/135\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.6108 - accuracy: 0.7939\n",
      "Epoch 44/135\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.6041 - accuracy: 0.7957\n",
      "Epoch 45/135\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.5977 - accuracy: 0.7979\n",
      "Epoch 46/135\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.5916 - accuracy: 0.8002\n",
      "Epoch 47/135\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.5857 - accuracy: 0.8021\n",
      "Epoch 48/135\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.5801 - accuracy: 0.8044\n",
      "Epoch 49/135\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.5747 - accuracy: 0.8066\n",
      "Epoch 50/135\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.5697 - accuracy: 0.8081\n",
      "Epoch 51/135\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.5648 - accuracy: 0.8094\n",
      "Epoch 52/135\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.5601 - accuracy: 0.8110\n",
      "Epoch 53/135\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.5557 - accuracy: 0.8127\n",
      "Epoch 54/135\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.5514 - accuracy: 0.8142\n",
      "Epoch 55/135\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.5472 - accuracy: 0.8151\n",
      "Epoch 56/135\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.5432 - accuracy: 0.8165\n",
      "Epoch 57/135\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.5393 - accuracy: 0.8177\n",
      "Epoch 58/135\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.5355 - accuracy: 0.8189\n",
      "Epoch 59/135\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.5319 - accuracy: 0.8199\n",
      "Epoch 60/135\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.5283 - accuracy: 0.8212\n",
      "Epoch 61/135\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.5249 - accuracy: 0.8221\n",
      "Epoch 62/135\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.5215 - accuracy: 0.8230\n",
      "Epoch 63/135\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.5183 - accuracy: 0.8239\n",
      "Epoch 64/135\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.5152 - accuracy: 0.8249\n",
      "Epoch 65/135\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.5121 - accuracy: 0.8256\n",
      "Epoch 66/135\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.5092 - accuracy: 0.8264\n",
      "Epoch 67/135\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.5063 - accuracy: 0.8273\n",
      "Epoch 68/135\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.5035 - accuracy: 0.8282\n",
      "Epoch 69/135\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.5008 - accuracy: 0.8291\n",
      "Epoch 70/135\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.4981 - accuracy: 0.8300\n",
      "Epoch 71/135\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.4956 - accuracy: 0.8308\n",
      "Epoch 72/135\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.4931 - accuracy: 0.8317\n",
      "Epoch 73/135\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.4907 - accuracy: 0.8328\n",
      "Epoch 74/135\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.4884 - accuracy: 0.8336\n",
      "Epoch 75/135\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.4865 - accuracy: 0.8338\n",
      "Epoch 76/135\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.4852 - accuracy: 0.8345\n",
      "Epoch 77/135\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.4841 - accuracy: 0.8342\n",
      "Epoch 78/135\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.4814 - accuracy: 0.8359\n",
      "Epoch 79/135\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.4777 - accuracy: 0.8364\n",
      "Epoch 80/135\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.4763 - accuracy: 0.8375\n",
      "Epoch 81/135\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.4755 - accuracy: 0.8377\n",
      "Epoch 82/135\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.4726 - accuracy: 0.8386\n",
      "Epoch 83/135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 161ms/step - loss: 0.4704 - accuracy: 0.8392\n",
      "Epoch 84/135\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.4697 - accuracy: 0.8393\n",
      "Epoch 85/135\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.4675 - accuracy: 0.8404\n",
      "Epoch 86/135\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.4652 - accuracy: 0.8408\n",
      "Epoch 87/135\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.4643 - accuracy: 0.8415\n",
      "Epoch 88/135\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.4626 - accuracy: 0.8421\n",
      "Epoch 89/135\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.4605 - accuracy: 0.8427\n",
      "Epoch 90/135\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.4593 - accuracy: 0.8431\n",
      "Epoch 91/135\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.4580 - accuracy: 0.8437\n",
      "Epoch 92/135\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.4561 - accuracy: 0.8443\n",
      "Epoch 93/135\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.4547 - accuracy: 0.8446\n",
      "Epoch 94/135\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.4536 - accuracy: 0.8450\n",
      "Epoch 95/135\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.4519 - accuracy: 0.8456\n",
      "Epoch 96/135\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.4504 - accuracy: 0.8461\n",
      "Epoch 97/135\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.4493 - accuracy: 0.8464\n",
      "Epoch 98/135\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.4480 - accuracy: 0.8464\n",
      "Epoch 99/135\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.4465 - accuracy: 0.8473\n",
      "Epoch 100/135\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.4453 - accuracy: 0.8476\n",
      "Epoch 101/135\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.4441 - accuracy: 0.8479\n",
      "Epoch 102/135\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.4427 - accuracy: 0.8485\n",
      "Epoch 103/135\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.4414 - accuracy: 0.8489\n",
      "Epoch 104/135\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.4403 - accuracy: 0.8490\n",
      "Epoch 105/135\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.4391 - accuracy: 0.8495\n",
      "Epoch 106/135\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.4378 - accuracy: 0.8502\n",
      "Epoch 107/135\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.4366 - accuracy: 0.8506\n",
      "Epoch 108/135\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.4355 - accuracy: 0.8510\n",
      "Epoch 109/135\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.4343 - accuracy: 0.8515\n",
      "Epoch 110/135\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.4331 - accuracy: 0.8519\n",
      "Epoch 111/135\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.4320 - accuracy: 0.8522\n",
      "Epoch 112/135\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.4309 - accuracy: 0.8525\n",
      "Epoch 113/135\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.4298 - accuracy: 0.8529\n",
      "Epoch 114/135\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.4286 - accuracy: 0.8531\n",
      "Epoch 115/135\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.4275 - accuracy: 0.8534\n",
      "Epoch 116/135\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.4265 - accuracy: 0.8537\n",
      "Epoch 117/135\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.4254 - accuracy: 0.8539\n",
      "Epoch 118/135\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.4244 - accuracy: 0.8544\n",
      "Epoch 119/135\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.4233 - accuracy: 0.8548\n",
      "Epoch 120/135\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.4224 - accuracy: 0.8550\n",
      "Epoch 121/135\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.4215 - accuracy: 0.8551\n",
      "Epoch 122/135\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.4208 - accuracy: 0.8557\n",
      "Epoch 123/135\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.4200 - accuracy: 0.8552\n",
      "Epoch 124/135\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.4191 - accuracy: 0.8559\n",
      "Epoch 125/135\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.4179 - accuracy: 0.8557\n",
      "Epoch 126/135\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.4166 - accuracy: 0.8569\n",
      "Epoch 127/135\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.4156 - accuracy: 0.8567\n",
      "Epoch 128/135\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.4147 - accuracy: 0.8571\n",
      "Epoch 129/135\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.4138 - accuracy: 0.8577\n",
      "Epoch 130/135\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.4130 - accuracy: 0.8577\n",
      "Epoch 131/135\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.4121 - accuracy: 0.8583\n",
      "Epoch 132/135\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.4112 - accuracy: 0.8585\n",
      "Epoch 133/135\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.4101 - accuracy: 0.8590\n",
      "Epoch 134/135\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.4091 - accuracy: 0.8593\n",
      "Epoch 135/135\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.4083 - accuracy: 0.8595\n",
      "Execution time: 21.914703130722046 seconds\n"
     ]
    }
   ],
   "source": [
    "model_wihtout_hm = get_model() \n",
    "st = time.time()\n",
    "model_wihtout_hm.fit(X_train_normalized, y_train, epochs = 135, verbose=1, callbacks=[CustomCallbackGeneric(), logger_generic_model], batch_size = X_train.shape[0]) \n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HM based optimizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/94\n",
      "1/1 [==============================] - 1s 724ms/step - loss: 2.3155 - accuracy: 0.1137\n",
      "Epoch 2/94\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1272 - accuracy: 0.2892\n",
      "Epoch 3/94\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 1.9921 - accuracy: 0.3880\n",
      "Epoch 4/94\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 1.8600 - accuracy: 0.4850\n",
      "Epoch 5/94\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 1.7271 - accuracy: 0.5570\n",
      "Epoch 6/94\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 1.5969 - accuracy: 0.5748\n",
      "Epoch 7/94\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 1.4724 - accuracy: 0.5831\n",
      "Epoch 8/94\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 1.3568 - accuracy: 0.5985\n",
      "Epoch 9/94\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 1.2530 - accuracy: 0.6238\n",
      "Epoch 10/94\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 1.1618 - accuracy: 0.6554\n",
      "Epoch 11/94\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 1.0837 - accuracy: 0.6752\n",
      "Epoch 12/94\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 1.0194 - accuracy: 0.6811\n",
      "Epoch 13/94\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.9661 - accuracy: 0.6861\n",
      "Epoch 14/94\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.9184 - accuracy: 0.6946\n",
      "Epoch 15/94\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.8773 - accuracy: 0.7020\n",
      "Epoch 16/94\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.8438 - accuracy: 0.7081\n",
      "Epoch 17/94\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.8151 - accuracy: 0.7145\n",
      "Epoch 18/94\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.7896 - accuracy: 0.7229\n",
      "Epoch 19/94\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.7678 - accuracy: 0.7287\n",
      "Epoch 20/94\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.7484 - accuracy: 0.7357\n",
      "Epoch 21/94\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.7305 - accuracy: 0.7444\n",
      "Epoch 22/94\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.7146 - accuracy: 0.7518\n",
      "Epoch 23/94\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.6990 - accuracy: 0.7585\n",
      "Epoch 24/94\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.6842 - accuracy: 0.7642\n",
      "Epoch 25/94\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.6711 - accuracy: 0.7703\n",
      "Epoch 26/94\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.6583 - accuracy: 0.7758\n",
      "Epoch 27/94\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.6463 - accuracy: 0.7812\n",
      "Epoch 28/94\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.6354 - accuracy: 0.7859\n",
      "Epoch 29/94\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.6249 - accuracy: 0.7897\n",
      "Epoch 30/94\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.6152 - accuracy: 0.7927\n",
      "Epoch 31/94\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.6059 - accuracy: 0.7971\n",
      "Epoch 32/94\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.5971 - accuracy: 0.8006\n",
      "Epoch 33/94\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.5890 - accuracy: 0.8029\n",
      "Epoch 34/94\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.5811 - accuracy: 0.8053\n",
      "Epoch 35/94\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.5744 - accuracy: 0.8064\n",
      "Epoch 36/94\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.5685 - accuracy: 0.8091\n",
      "Epoch 37/94\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.5626 - accuracy: 0.8108\n",
      "Epoch 38/94\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.5549 - accuracy: 0.8137\n",
      "Epoch 39/94\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.5474 - accuracy: 0.8156\n",
      "Epoch 40/94\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.5426 - accuracy: 0.8168\n",
      "Epoch 41/94\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.5375 - accuracy: 0.8179\n",
      "Epoch 42/94\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.5311 - accuracy: 0.8200\n",
      "Epoch 43/94\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.5260 - accuracy: 0.8221\n",
      "Epoch 44/94\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.5218 - accuracy: 0.8217\n",
      "Epoch 45/94\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.5168 - accuracy: 0.8234\n",
      "Epoch 46/94\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.5123 - accuracy: 0.8252\n",
      "Epoch 47/94\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.5088 - accuracy: 0.8249\n",
      "Epoch 48/94\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.5044 - accuracy: 0.8271\n",
      "Epoch 49/94\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.5002 - accuracy: 0.8287\n",
      "Epoch 50/94\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.4970 - accuracy: 0.8282\n",
      "Epoch 51/94\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.4933 - accuracy: 0.8313\n",
      "Epoch 52/94\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.4894 - accuracy: 0.8321\n",
      "Epoch 53/94\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.4860 - accuracy: 0.8329\n",
      "Epoch 54/94\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.4831 - accuracy: 0.8356\n",
      "Epoch 55/94\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.4798 - accuracy: 0.8352\n",
      "Epoch 56/94\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.4764 - accuracy: 0.8374\n",
      "Epoch 57/94\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.4736 - accuracy: 0.8384\n",
      "Epoch 58/94\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.4710 - accuracy: 0.8382\n",
      "Epoch 59/94\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.4683 - accuracy: 0.8401\n",
      "Epoch 60/94\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.4655 - accuracy: 0.8401\n",
      "Epoch 61/94\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.4628 - accuracy: 0.8415\n",
      "Epoch 62/94\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.4602 - accuracy: 0.8425\n",
      "Epoch 63/94\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.4580 - accuracy: 0.8425\n",
      "Epoch 64/94\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.4558 - accuracy: 0.8444\n",
      "Epoch 65/94\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.4538 - accuracy: 0.8435\n",
      "Epoch 66/94\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.4516 - accuracy: 0.8452\n",
      "Epoch 67/94\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.4496 - accuracy: 0.8441\n",
      "Epoch 68/94\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.4473 - accuracy: 0.8470\n",
      "Epoch 69/94\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.4450 - accuracy: 0.8464\n",
      "Epoch 70/94\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.4427 - accuracy: 0.8483\n",
      "Epoch 71/94\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.4407 - accuracy: 0.8486\n",
      "Epoch 72/94\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.4388 - accuracy: 0.8493\n",
      "Epoch 73/94\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.4372 - accuracy: 0.8496\n",
      "Epoch 74/94\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.4355 - accuracy: 0.8497\n",
      "Epoch 75/94\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.4338 - accuracy: 0.8510\n",
      "Epoch 76/94\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.4322 - accuracy: 0.8509\n",
      "Epoch 77/94\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.4307 - accuracy: 0.8522\n",
      "Epoch 78/94\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.4292 - accuracy: 0.8520\n",
      "Epoch 79/94\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.4277 - accuracy: 0.8530\n",
      "Epoch 80/94\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.4260 - accuracy: 0.8529\n",
      "Epoch 81/94\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.4242 - accuracy: 0.8544\n",
      "Epoch 82/94\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.4223 - accuracy: 0.8548\n",
      "Epoch 83/94\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.4207 - accuracy: 0.8554\n",
      "Epoch 84/94\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.4192 - accuracy: 0.8558\n",
      "Epoch 85/94\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.4179 - accuracy: 0.8561\n",
      "Epoch 86/94\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.4167 - accuracy: 0.8568\n",
      "Epoch 87/94\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.4154 - accuracy: 0.8566\n",
      "Epoch 88/94\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.4141 - accuracy: 0.8575\n",
      "Epoch 89/94\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.4131 - accuracy: 0.8570\n",
      "Epoch 90/94\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.4123 - accuracy: 0.8578\n",
      "Epoch 91/94\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.4120 - accuracy: 0.8567\n",
      "Epoch 92/94\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.4111 - accuracy: 0.8574\n",
      "Epoch 93/94\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.4103 - accuracy: 0.8575\n",
      "Epoch 94/94\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.4074 - accuracy: 0.8590\n",
      "Execution time: 20.00899624824524 seconds\n"
     ]
    }
   ],
   "source": [
    "model_hm = get_model() \n",
    "st = time.time()\n",
    "model_hm.fit(X_train_normalized, y_train, epochs = 94, verbose=1, callbacks=[CustomCallbackHM(),logger_hm_model], batch_size = X_train.shape[0]) \n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_69 (Dense)            (None, 64)                50240     \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_hm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic opimizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 998us/step - loss: 0.4487 - accuracy: 0.8421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4487186670303345, 0.8421000242233276]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wihtout_hm.evaluate(X_test_normalized, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HM based optimizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 0.4434 - accuracy: 0.8449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4433857798576355, 0.8449000120162964]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_hm.evaluate(X_test_normalized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
